#!/usr/bin/env python

import json

import streams
import tracks
import users

from create_pipeline import CreatePipeline
from options import SetPipelineOptions

spo = SetPipelineOptions()
spo.options = spo.set_pipeline()
options = spo.set_google_cloud_options(spo.google_cloud, spo.options)
options = spo.set_runner(spo.options, spo.config.get('standard').get('runner'))
pipeline = CreatePipeline(options).pipeline

read_streams = streams.ReadStreams()

streams_pc = read_streams.read_streams(pipeline)
streams_pc = read_streams.map_streams(streams_pc)

print(streams_pc)

read_users = users.ReadUsers()

users_pc = read_users.read_users(pipeline)
users_pc = read_users.map_users(users_pc)

print(users_pc)

read_tracks = tracks.ReadTracks()

tracks_pc = read_tracks.read_tracks(pipeline)
tracks_pc = read_tracks.map_tracks(tracks_pc)

group_streams = streams.GroupStreams()

streams_pc = group_streams.group_streams_with_users(streams_pc, users_pc)

"""
s = s | 'process users' >> ab.ParDo(ProcessUsers())
s = s | 'remap streams' >> ab.Map(lambda sid: (sid.get('track_id'), sid))
s = (({'streams':s, 'tracks':t}) | 'co group by key tracks' >> ab.CoGroupByKey(pipeline=p))
s = s | 'process tracks' >> ab.ParDo(ProcessTracks())
#s = s | 'print st' >> ab.Map(lambda sid: print(sid))
s = s | 'output' >> ab.io.WriteToText('gs://abbynormal/dnormal')

p.run()
"""
