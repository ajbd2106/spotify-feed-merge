#!/usr/bin/env python

import apache_beam as ab
import json

print(__name__)

from apache_beam.pvalue import AsList

import tracks
import users

o = ab.utils.pipeline_options.PipelineOptions()
gco = o.view_as(ab.utils.pipeline_options.GoogleCloudOptions)
gco.project = 'umg-technical-evaluation'
gco.staging_location = 'gs://abbynormal-staging/stage'
gco.temp_location = 'gs://abbynormal-staging/tmp'
o.view_as(ab.utils.pipeline_options.StandardOptions).runner = 'DataflowRunner'

p = []
p = ab.Pipeline(options=o)
s = (p 
     | 'read streams' >> ab.io.ReadFromText('gs://abbynormal/streams.gz') 
     | 'map streams' >> ab.Map(lambda user_id: (json.loads(user_id).get('user_id'), json.loads(user_id)))
)

u = (p
     | 'read users' >> ab.io.ReadFromText('gs://abbynormal/users.gz')
     | 'map users' >> ab.Map(lambda user_id: (json.loads(user_id).get('user_id'), json.loads(user_id)))
)

t = (p
     | 'read tracks' >> ab.io.ReadFromText('gs://abbynormal/tracks.gz')
     | 'map tracks' >> ab.Map(lambda track_id: (json.loads(track_id).get('track_id'), json.loads(track_id)))
)

s = (({'streams':s,'users':u}) | 'co group by key users' >> ab.CoGroupByKey())
s = s | 'process users' >> ab.ParDo(ProcessUsers())
s = s | 'remap streams' >> ab.Map(lambda sid: (sid.get('track_id'), sid))
s = (({'streams':s, 'tracks':t}) | 'co group by key tracks' >> ab.CoGroupByKey(pipeline=p))
s = s | 'process tracks' >> ab.ParDo(ProcessTracks())
#s = s | 'print st' >> ab.Map(lambda sid: print(sid))
s = s | 'output' >> ab.io.WriteToText('gs://abbynormal/dnormal')

p.run()
